{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Internal DIOL wiki","text":""},{"location":"#welcome-to-the-distributed-intelligence-and-optimization-lab-diol-wiki","title":"Welcome to the Distributed Intelligence and Optimization Lab (DIOL) Wiki!","text":"<p>Whether you're a seasoned researcher or a newcomer, we're thrilled to have you here. This wiki is a collaborative space for sharing knowledge, insights, and best practices within our diverse and dynamic laboratory.</p> <p>We encourage you to contribute by writing guides on anything you find valuable \u2013 from optimizing algorithms to streamlining workflows. Your expertise is not only valuable to you but can greatly benefit others in our community. Let's build a repository of collective wisdom that propels us all forward.</p> <p>To get started, check out the \"How To\" page where you can find a guide on how to write and deploy your contibution. Feel free to explore, contribute, and make this wiki a hub of shared intelligence. Happy writing!  </p>"},{"location":"how-to-wiki/","title":"How to write on the internal wiki","text":"<p>Welcome!  If you've found your way here, chances are you're interested in contributing to this wiki or perhaps just exploring. If it's the former, congratulations \u2013 you're in the right place!  </p> <p>If it's the latter, feel free to take a look around; you might discover something you enjoy.  And who knows, by the end, you might find yourself inspired to contribute to this wiki as well. </p>"},{"location":"how-to-wiki/#get-started","title":"Get started","text":"<p>To begin, head to the wiki repository and follow the instructions outlined in the README to clone and set up the repository on your PC. Afterward, return here to explore the folder and file structure, grasp the basics of writing in markdown, and deploy your first guide on this internal wiki.</p>"},{"location":"how-to-wiki/#file-and-folder-structure","title":"File and Folder structure","text":"<p>This is the file and folder structure. Take a look at the <code>mkdocs.yml</code> file \u2013 it's the mastermind behind the wiki structure, allowing you to specify its skeleton. Next up is the <code>docs/</code> folder, home to all the markdown files. Feel free to add yours if the need arises. You'll also come across the <code>.github/workflows/</code> folder \u2013 just give it a pass, and don't mind the <code>.gitignore</code>, <code>README.md</code>, <code>LICENSE</code>, and <code>requirements.txt</code> files.</p> <pre><code>.github/\n    workflows/\n        ci.yml\ndocs/\n    index.md\n    how-to-wiki.md\n    server-guide/\n        slurm.md\n        ...\n    ...\n.gitignore\nLICENSE\nmkdocs.yml\nREADME.md\nrequirements.txt\n</code></pre>"},{"location":"how-to-wiki/#the-mkdocsyml-file","title":"The mkdocs.yml file","text":"<p>Delving into the <code>mkdocs.yml</code> file, pay particular attention to the <code>nav</code> section. Here, you'll have the opportunity to intricately design the structure of the tabs at the top of the wiki and fine-tune the navigation within them. This is where you can sculpt a user-friendly experience, ensuring a well-organized and seamless interface for your readers.</p> <pre><code>nav:\n  - Home: index.md\n  - How to wiki: how-to-wiki.md\n  - Server: \n    - server-guide/index.md\n    - Slurm: server-guide/slurm.md\n</code></pre> <p>Upon examination of this code snippet, it becomes evident that we have three tabs \u2013 namely, \"Home,\" \"How to Wiki,\" and \"Server.\" Notably, the \"Server\" tab reveals additional subpages underneath it. Delving into this tab provides the flexibility to choose a specific sub-topic for a more focused exploration</p> <p>For instance, if you wish to include a page under the 'Server' tab, simply add a markdown file within the 'server-guide/' folder:</p> <pre><code>...\ndocs/\n    index.md\n    how-to-wiki.md\n    server-guide/\n        slurm.md\n        new-page.md\n        ...\n    ...\n...\n</code></pre> <p>If you intend to introduce a new tab labeled 'Transportation' and populate it with two pages for guides on 'How to Sit on Bus No. 5' and 'Forecasting of Buses Deviation,' the updated code would be: <pre><code>nav:\n  - Home: index.md\n  - How to wiki: how-to-wiki.md\n  - Server: \n    - server-guide/index.md\n    - Slurm: server-guide/slurm.md\n  - Transportation: \n    - How to sit on Bus No. 5: transportation/sit-on-five.md\n    - Forecasting of Buses Deviation: transportation/bus-deviation.md\n</code></pre> and the new file structure will be: <pre><code>docs/\n    index.md\n    how-to-wiki.md\n    server-guide/\n        ...\n    tansportation/\n        sit-on-five.md\n        bus-deviation.md\n    ...\n</code></pre></p>"},{"location":"how-to-wiki/#markdown-basics","title":"Markdown basics","text":"<p>Here, you'll discover fundamental markdown guidelines to commence your wiki contributions. Given the nature of likely contributions, such as guiding installations of libraries or ensuring code functionality, the emphasis will primarily be on effectively articulating code in markdown. For full documentation visit mkdocs.org and Material for MkDocs</p>"},{"location":"how-to-wiki/#code-blocks","title":"Code blocks","text":"<p>Here the markdown code for a simple code block, where \"language\" is the actual language of the code snippet you're writing.  <pre><code>```language\nbool get_buses_deviation(bool christmas_markets){\n    if(christmas_markets){\n        return true;\n    }\n    return false;\n}\n```\n</code></pre> The rendered output using \"C++\" as language will look like this: <pre><code>bool get_buses_deviation(bool christmas_markets){\n    if(christmas_markets){\n        return true;\n    }\n    return false;\n}\n</code></pre></p> <p>If you want to highlight some lines of code, you'll have to add <code>hl_lines=\"line numbers\"</code> like this: <pre><code>```c++ hl_lines=\"3 4\"\nbool get_buses_deviation(bool christmas_markets){\n    if(christmas_markets){\n        std::cout &lt;&lt; \"People are freezing in Piazza Fiera, ignoring the bus will never pass \" &lt;&lt; std::endl;\n        send_help();\n        return true;\n    }\n    return false;\n}\n```\n</code></pre> Rendered output: <pre><code>bool get_buses_deviation(bool christmas_markets){\n    if(christmas_markets){\n        std::cout &lt;&lt; \"People are freezing in Piazza Fiera, ignoring the bus will never pass \" &lt;&lt; std::endl;\n        send_help();\n        return true;\n    }\n    return false;\n}\n</code></pre></p> <p>You can also add the title and line numbers: <pre><code>```py title=\"transports.py\" linenums=\"1\"\ndef get_buses_deviation(christmas_markets):\n    if(christmas_markets):\n        return true\n    return false\n```\n</code></pre></p> <p>Rendered output:</p> transports.py<pre><code>def get_buses_deviation(christmas_markets):\n    if(christmas_markets):\n        return true\n    return false\n</code></pre>"},{"location":"how-to-wiki/#add-tables","title":"Add tables","text":"<p>Here you can find the code snippet for creating tables. You can either write it in a pretty or raw way, the rendered result will always be the same: Pretty<pre><code>| Bus stop             | Time Arrival  |\n|----------------------|---------------|\n| Povo Valoni          | 08:43         |\n| Povo Facolt\u00e0 scienze | 08:44         |\n</code></pre> Raw<pre><code>| Bus stop | Time Arrival |\n|----------|---------------|\n| Povo Valoni | 08:43 |\n| Povo Facolt\u00e0 scienze | 08:44 |\n</code></pre> Rendered output:</p> Bus stop Time Arrival Povo Valoni 08:43 Povo Facolt\u00e0 scienze 08:44"},{"location":"how-to-wiki/#add-images","title":"Add images","text":"<p>To incorporate images, simply upload them to the <code>docs/img/</code> folder. Feel free to organize your images into subfolders for better management. Subsequently, you can integrate the image into your content using the provided code, replacing 'your_image.jpeg' with the respective file name.</p> <pre><code>&lt;img alt=\"image placeholder\" src=\"../img/your_image.jpeg\" width='250'&gt;\n</code></pre> <p>For example, you can find here an image of a cute capibara peluche to bright your day!</p> <p></p>"},{"location":"how-to-wiki/#admonition","title":"Admonition","text":"<p>Additionally, you have the option to incorporate aesthetically styled admonitions. Feel free to scroll through the guide to explore various types available for enhancing the presentation of your content. Here is the markdown code snippet: <pre><code>!!! note\n    blablabla\n</code></pre></p> <p>Below you can find all the types rendered:</p> <p><code>note</code></p> <p>Note</p> <p>blablabla</p> <p><code>abstract</code></p> <p>Abstract</p> <p>blablabla</p> <p><code>tip</code></p> <p>Tip</p> <p>blablabla</p> <p><code>success</code></p> <p>Success</p> <p>blablabla</p> <p><code>question</code></p> <p>Question</p> <p>blablabla?</p> <p><code>warning</code></p> <p>Warning</p> <p>blablabla!</p> <p><code>failure</code></p> <p>Failure</p> <p>blablabla</p> <p><code>danger</code></p> <p>Danger</p> <p>blablabla</p> <p><code>bug</code></p> <p>Bug</p> <p>blablabla</p> <p><code>example</code></p> <p>Example</p> <p>blablabla</p> <p><code>quote</code></p> <p>Quote</p> <p>\"blablabla\"</p>"},{"location":"how-to-wiki/#deploy-your-contribution","title":"Deploy your contribution","text":"<p>Once you've finished your valuable contribution to this wiki, the next step is to deploy it to the site. Don't be scared, the process is straightforward! \ud83d\ude04</p> <p>All you need to do is commit and push your new or updated files to the GitHub repository. In case you're not familiar with Git, you'll find the terminal commands below for your convenience. For obvious reasons edit the message \"New guide on how to do things\" with something more descriptive of what you've done.</p> <pre><code>git add .\ngit commit -m \"New guide on how to do things\"\ngit push origin main\n</code></pre> <p>Following the push to the main branch, a bit of magic will take place, and in no time, your modifications will be live. Simply refresh the page to admire your splendid work! \ud83d\udc4f</p>"},{"location":"or-tools/","title":"OR Tools","text":"<p>This section will contain guides on how to use or tools for topics for which official guides are missing</p>"},{"location":"or-tools/CPLEX-python/","title":"Building OR-Tools with CPLEX Support on Mac M1","text":"<p>This guide documents the steps to build OR-Tools with CPLEX support on a Mac M1 and install it in a Conda environment.</p>"},{"location":"or-tools/CPLEX-python/#1-clone-the-or-tools-repository","title":"1. Clone the OR-Tools Repository","text":"<pre><code>git clone https://github.com/google/or-tools.git\ncd or-tools\n</code></pre>"},{"location":"or-tools/CPLEX-python/#2-modify-findcplexcmake","title":"2. Modify <code>FindCPLEX.cmake</code>","text":"<p>Edit the file <code>cmake/dependencies/FindCPLEX.cmake</code>: - Remove the following line (around line 78):   <pre><code>message(FATAL_ERROR \"CPLEX do not support Apple M1, can't find a suitable static library\")\n</code></pre> - Add the following lines to correctly set the CPLEX target properties:   <pre><code>set_target_properties(CPLEX::CPLEX PROPERTIES\n    IMPORTED_LOCATION \"${CPLEX_ROOT}/cplex/lib/arm64_osx/static_pic/libcplex.a\")\n</code></pre></p>"},{"location":"or-tools/CPLEX-python/#3-configure-cmake","title":"3. Configure CMake","text":"<p>Open a shell and run: <pre><code>export CPLEX_ROOT=\"/Applications/CPLEX_Studio2211\"\nexport UNIX_CPLEX_DIR=\"$CPLEX_ROOT\"\nexport LDFLAGS=\"-L$CPLEX_ROOT/cplex/lib/arm64_osx/static_pic -L$CPLEX_ROOT/cplex/lib/arm64_osx -lz\"\nexport PATH=\"$HOME/.local/bin:$PATH\"  # Ensure stubgen is found\n\ncmake -S . -B build -DBUILD_DEPS:BOOL=ON -DUSE_CPLEX=ON -DCPLEX_ROOT=\"$CPLEX_ROOT\" -DBUILD_PYTHON:BOOL=ON\n</code></pre></p>"},{"location":"or-tools/CPLEX-python/#4-build-or-tools","title":"4. Build OR-Tools","text":"<pre><code>cmake --build build --config Release\n</code></pre>"},{"location":"or-tools/CPLEX-python/#5-test-the-virtual-environment","title":"5. Test the Virtual Environment","text":"<p>OR-Tools automatically installs into a virtual environment. Activate it: <pre><code>source build/python/venv/bin/activate\n</code></pre> Then, check if OR-Tools is installed and whether CPLEX is available: <pre><code>python -c \"import ortools.linear_solver.pywraplp; print('ORTools installed successfully!')\"\n</code></pre> You should see 'ORTools installed successfully!'.</p> <p>Create a test.py file and copy this code:</p> <pre><code>from ortools.linear_solver import pywraplp\n\nsolver = pywraplp.Solver.CreateSolver(\"CPLEX\")\nif solver is not None:\n    print(\"CPLEX is available!\")\nelse:\n    print(\"CPLEX is NOT available.\")\n</code></pre> <p>then in the shell:</p> <pre><code>python test.py\n</code></pre> <p>you should see 'CPLEX is available!'</p>"},{"location":"or-tools/CPLEX-python/#6-install-or-tools-in-a-conda-environment","title":"6. Install OR-Tools in a Conda Environment","text":"<pre><code>conda create -n Myenv python=3.11\nconda activate Myenv\npip install build/python/dist/ortools-*.whl\n</code></pre>"},{"location":"or-tools/CPLEX-python/#7-verify-installation-in-conda","title":"7. Verify Installation in Conda","text":"<p>Try again the steps at point 5.</p> <p>If everything works correctly, OR-Tools with CPLEX is now installed in the Conda environment!</p>"},{"location":"or-tools/CPLEX-python/#notes","title":"Notes","text":"<ul> <li>Ensure <code>CPLEX_ROOT</code> is correctly set before running CMake.</li> <li>If you encounter linker errors, double-check the <code>LDFLAGS</code> export for proper library paths.</li> <li>The OR-Tools build process installs dependencies in <code>build/python/venv</code>, so testing inside the virtual environment first can help diagnose issues.</li> </ul>"},{"location":"server-guide/","title":"Server usage guides","text":"<p>Greetings! Welcome to the Server Usage Guide, your hub for optimizing server usage. Delve into guides on utilizing libraries, refining workflows, and exploring the diverse capabilities of our server. This page serves as the go-to resource for sharing valuable tips and tricks, so don't hesitate to contribute your insights. Together, let's streamline and enhance our server experience!</p>"},{"location":"server-guide/ctransformers-library/","title":"Guide for CTransformers library usage on server 190","text":"<p>Since the server 190 has CUDA 11 installed, while the ctransformers library wants CUDA 12, here you can find a guide on how to get it to work anyway.</p> <p>First thing first, try typing:</p> <p><pre><code>nvcc --version\n</code></pre> If the answer is like </p> <pre><code>Command 'nvcc' not found, but can be installed with:\n\napt install nvidia-cuda-toolkit\nPlease ask your administrator\n</code></pre> <p>then type: <pre><code>nano .bashrc\n</code></pre></p> <p>and add this lines at the end of the file</p> <pre><code>export CUDA_HOME=/usr/local/cuda\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\nexport PATH=$PATH:$CUDA_HOME/bin\n</code></pre> <p>save the file and run:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Then try again:</p> <pre><code>nvcc --version\n</code></pre> <p>You should get an answer like this:</p> <pre><code>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Jun__8_16:49:14_PDT_2022\nCuda compilation tools, release 11.7, V11.7.99\nBuild cuda_11.7.r11.7/compiler.31442593_0\n</code></pre> <p>No follow this github comment to compile ctransformers library for CUDA 11.</p>"},{"location":"server-guide/slurm/","title":"Slurm short guide","text":""},{"location":"server-guide/slurm/#what-is-it","title":"What is it","text":"<p>From slurm website</p> <p>Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. As a cluster workload manager, Slurm has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.</p> <p>Choosing Slurm just makes sense. It's not just a workload manager; it's your partner in getting stuff done efficiently. Imagine a world where you don't have to worry about resource chaos and can navigate through your computational tasks effortlessly. Slurm not only streamlines your workflow but also empowers you to make the most out of your computing infrastructure. So why settle for anything less? Make Slurm your ally, and let's take your high-performance computing experience to the next level!</p>"},{"location":"server-guide/slurm/#how-to-use-it","title":"How to use it","text":""},{"location":"server-guide/slurm/#step-1-understanding-the-basics","title":"Step 1: Understanding the Basics","text":"<p>Before diving in, familiarize yourself with the fundamental concepts of Slurm:</p> <ul> <li>Job: A unit of work scheduled by Slurm.</li> <li>Partition: A grouping of computing resources, allowing you to allocate tasks strategically.</li> <li>Node: An individual computer in the cluster.</li> </ul>"},{"location":"server-guide/slurm/#step-2-checking-the-cluster-status","title":"Step 2: Checking the Cluster Status","text":"<p>To see the current status of the cluster, use the following command:</p> <pre><code>sinfo\n</code></pre> <p>This will display information about partitions, nodes, and their availability.</p>"},{"location":"server-guide/slurm/#step-3-submitting-a-job","title":"Step 3: Submitting a Job","text":"<p>Prepare a script containing your computational tasks, for example, a bash script named <code>my_script.sh</code>. Then, submit it to Slurm using:</p> <pre><code>sbatch my_script.sh\n</code></pre> <p>An example of the bash script could be:</p> <pre><code>#!/bin/bash\n#SBATCH -c 1\n#SBATCH -n 10\n#SBATCH -t 150:00:00\n#SBATCH -p LocalQ\n#SBATCH -o /home/your_user/my_folder/runs/test.out \n#SBATCH -e /home/your_user/my_folder/error_logs/test_error.out\n\nsource my_venv/bin/activate\ncd my_folder/ \npython3 main.py\n</code></pre> <p>Here below the explanation of the script, for more info follow the slurm official page:</p> <ul> <li>Line 2: Advise the Slurm controller that ensuing job steps will require ncpus (in this case 1) number of processors per task. Without this option, the controller will just try to allocate one processor per task.</li> <li>Line 3: This option advises the Slurm controller that job steps run within the allocation will launch a maximum of number (in this case 10) tasks and to provide for sufficient resources.</li> <li>Line 4: Set a limit on the total run time of the job allocation. If the requested time limit exceeds the partition's time limit, the job will be left in a PENDING state (possibly indefinitely). The default time limit is the partition's default time limit.</li> <li>Line 5: Request a specific partition for the resource allocation. If not specified, the default behavior is to allow the slurm controller to select the default partition as designated by the system administrator.</li> <li>Line 6: Instruct Slurm to connect the batch script's standard output directly to the file name specified</li> <li>Line 7: Instruct Slurm to connect the batch script's standard error directly to the file name specified</li> <li>Line 8-10: Actual bash script</li> </ul>"},{"location":"server-guide/slurm/#step-4-monitoring-jobs","title":"Step 4: Monitoring Jobs","text":"<p>Monitor all jobs using: <pre><code>squeue\n</code></pre> or your jobs: <pre><code>squeue -u your_username\n</code></pre></p> <p>This will show the status of your submitted jobs.</p>"},{"location":"server-guide/slurm/#step-5-canceling-a-job","title":"Step 5: Canceling a Job","text":"<p>To cancel a job, simply use:</p> <pre><code>scancel &lt;your_job_id&gt;\n</code></pre> <p>Replace <code>&lt;your_job_id&gt;</code> with the ID of the job you want to cancel.</p>"},{"location":"server-guide/slurm/#submit-your-first-job","title":"Submit your first job","text":"<p>Here a tutorial to submit your first job! Connect to the server and start the tutorial.</p> <p>Create a very simple python script in a custom folder <code>my_folder</code> and call it <code>my_py_script.py</code>:</p> <pre><code>mkdir my_folder\ncd my_folder\nnano my_py_script.py\n</code></pre> <p>copy and paste this toy code:</p> my_py_script.py<pre><code>import time\n\ntime.sleep(60)\nprint(\"Hello world!\")\n</code></pre> <p>Note</p> <p>Note that the script wait 1 minute in order to let you have the time to see it in the queue</p> <p>Then go back to your home and create a bash script and call it <code>my_bash.sh</code></p> <pre><code>cd ..\nnano my_bash.sh\n</code></pre> <p>copy and paste this code:</p> <pre><code>#!/bin/bash\n#SBATCH -o /home/your_username/my_folder/test.out\n#SBATCH -e /home/your_username/my_folder/error_test.out\n\npython3 my_folder/my_py_script.py\n</code></pre> <p>Now submit your bash script to slurm:</p> <pre><code>sbatch my_bash.sh\n</code></pre> <p>And monitor it in the queue:</p> <pre><code>squeue\n</code></pre> <p>After a short interval (approximately 1 minute, taking into account the execution time of the Python script), you will observe the appearance of the phrase \"Hello world!\" in the <code>test.out</code> file. Check it with:</p> <pre><code>cat my_folder/test.out\n</code></pre> <p>Congrats, you've submitted and monitored your first slurm job!</p>"},{"location":"server-guide/start-kill-ollama/","title":"How to kill and restart ollama","text":""},{"location":"server-guide/start-kill-ollama/#kill","title":"Kill","text":"<pre><code>systemctl stop ollama.service\n</code></pre>"},{"location":"server-guide/start-kill-ollama/#restart","title":"Restart","text":"<pre><code>systemctl start ollama.service\n</code></pre>"},{"location":"workflow-management/","title":"Workflow management","text":"<p>This section will contain guides on how to manage experimentations using varying  frameworks or tools on different platforms.</p>"},{"location":"workflow-management/hydra-mlflow-python/","title":"Configuring with Hydra and logging on MLflow","text":"<p>This guide documents the steps to use Hydra and to log your results on Dagshub's MLflow  server on python. Note that this section includes many of my personal preferences. Feel  free to share your ideas in case you find them more efficient or better. First I will walk you through Hydra on clusters. Then, Dagshub's MLflow server and connecting it with Hydra.</p>"},{"location":"workflow-management/hydra-mlflow-python/#1-hydra","title":"1. Hydra","text":"<p>Hydra is an open-source Python framework that simplifies the development of research and  other complex applications, which is what we often work on, having many different  experiments with varying configurations in a single project. And yes, hydra simplifies  that. I won't mention about how everything works on Hydra since there is a beautiful guide on their website. Instead, I will mention about  what might really be useful for us such as running multiple experiments in parallel on  Slurm or HPC cluster with a single line.</p> <p>I have a template I use as the base of every project I have on berab/hydra_template. I will consider an even  more simplified project in each section. </p> <p>A very simple main source file <code>src/main.py</code> looks like: <pre><code>import hydra\nfrom omegaconf import DictConfig\nfrom dataclasses import dataclass\nfrom hydra.utils import instantiate\n\n@dataclass\nclass Main:\n    proj_name: str\n    seed: int\n    debug: int\n\n@hydra.main(config_path=\"../conf/\", config_name=\"main\", version_base='1.2')\ndef main(cfg: DictConfig):\n    cfg = instantiate(cfg)\n    print(f'Configured with seed {cfg.seed}!')\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> And its corresponding config file <code>conf/main.yaml</code> looks like: <pre><code>_target_: main.Main\nproj_name: hydra_template\ndefaults:\n  - _self_\n\nseed: 47\ndebug: 0\n</code></pre></p>"},{"location":"workflow-management/hydra-mlflow-python/#11-hydra-on-slurm","title":"1.1. Hydra on Slurm","text":"<p>with hydra you can run multiple experiments swept on <code>seed</code> and <code>debug</code> arguments with the line: <pre><code>python src/main.py --multirun seed=45,12,51 debug=0,1\n</code></pre> This would run 6 (3x2) experiments sequentially. To run those on Slurm in parallel, we can add another config file <code>slurm.yaml</code>: <pre><code>defaults:\n  - override hydra/sweeper: submitit_slurm\nhydra:\n  launcher:\n    partition: long-disi\n    timeout_min: 2880\n    nodes: 1\n    tasks_per_node: 1\n    cpus_per_task: 8\n    mem_per_cpu: 2048M\n    gres: gpu:1\n</code></pre></p> <p>Then you should also add this configuration to your main config file <code>conf/main.yaml</code>: <pre><code>...\ndefaults:\n  - _self_\n  - slurm\n...\n</code></pre></p> <p>This is the default configuration I often use but you can change it however you want. After, you can run multiple experiments <pre><code>python src/main.py --multirun seed=45,12,51\n</code></pre> Note that for this to run, we should install another python package you can find in <code>requirements.txt</code> in hydra_template repository.</p>"},{"location":"workflow-management/hydra-mlflow-python/#12-hydra-on-hpc","title":"1.2. Hydra on HPC","text":"<p>UniTN HPC cluster uses PBS launcher which Hydra doesn't support. I wrote a  code merging pbs4py framework and hydra's sweeper. You can find the code repository and how to install it here.</p> <p>After the installation, you should create a config file <code>pbs.yaml</code>: <pre><code>defaults:\n  - override hydra/sweeper: pbs\nhydra:\n  sweeper:\n    job_name: ${proj_name}\n    queue_name: common_cpuQ # (str) \u2013 Queue name which goes on the \u201c#PBS -N {name}\u201d line of the pbs header)\n    ncpus_per_node: 1 # (int) \u2013 Number of CPU cores per node\n    ngpus_per_node: 0 # (int) \u2013 Number of GPUs per node\n    queue_node_limit: 10 # (int) \u2013 Maximum number of nodes allowed in this queue\n    time: 8 # (int) \u2013 The requested job walltime in hours\n    mem: 16 #  (str) \u2013 The requested memory size. String to allow specifying in G, MB, etc.\n    profile_file: ~/.bashrc # Sources before starting, (str) \u2013 The file setting the environment to source inside the PBS job.  Set to \u2018\u2019 if you do not wish to source a file.\n    requested_number_of_nodes: 1 # (int) \u2013 The number of compute nodes to request\n</code></pre> Further information about the configurations of the PBS can be found in pbs4py. Again, your main config file <code>conf/main.yaml</code> should be: <pre><code>...\ndefaults:\n  - _self_\n  - pbs\n...\n</code></pre> Again, you should change the way you need <code>pbs.yaml</code>. I activate my python environment in <code>~/.bashrc</code>, but if you don't do that there, you should add this bash file  to <code>profile_file</code>. I also have a script because of the queue limit (30) of HPC server so that if the scripts  exceeds the limit, it waits n seconds and check until the limit is sufficient.  In hpc server, having the script <code>wait_limit.sh</code>: <pre><code>while [ $(qstat | grep renanberan.kilic | wc -l) -gt 24 ]; do\n  echo \"waiting for 30m\"\n  sleep 1800  # Wait before submitting the next batch\ndone\necho \"A job is starting\"\npython src/main.py --multirun seed=47,23,14 debug=0,1\n</code></pre></p> <p>How to run: <pre><code>nohup bash wait_limit.sh &gt; wait.out &amp;\n</code></pre></p>"},{"location":"workflow-management/hydra-mlflow-python/#2-dagshubs-mlflow-server","title":"2. Dagshub's MLflow server","text":"<p>I use Dagshub's MLflow for logging the results of my  experiments. Dagshub have a free MLflow server and you can also connect Github  repositories to Dagshub if needed to use its server.</p>"},{"location":"workflow-management/hydra-mlflow-python/#21-connecting-mlflow-and-hydra","title":"2.1. Connecting MLflow and Hydra","text":"<p>After having a Dagshub repository we can use its name and your account's token to  connect and log your results. But first, you should create an MLflow experiment on your Dagshub's MLflow page.  For me, it is https://dagshub.com/berab/hydra_template.mlflow. This will create a new experiment with id. We will use this id later on in our source code. For these infos, i create another config file <code>dagshub.yaml</code>: <pre><code>username: berab\ntoken: my_token\n</code></pre> and adding this to your main config: Then you should also add this configuration to your main config file <code>conf/main.yaml</code>: <pre><code>...\ndefaults:\n  - _self_\n  - dagshub\n...\n</code></pre></p> <p>And in your source code <code>src/main.py</code>, you can connect to Dagshub's MLflow server as: <pre><code>...\nimport dagshub\nimport mlflow\n\n@dataclass\nclass Main:\n    ...\n    username: str\n    token: str\n\n@hydra.main(config_path=\"../conf/\", config_name=\"main\", version_base='1.2')\ndef main(cfg: DictConfig):\n    cfg = instantiate(cfg)\n\n    dagshub.init(cfg.proj_name, cfg.username, mlflow=True)\n    mlflow.environment_variables.MLFLOW_TRACKING_PASSWORD = cfg.token\n    mlflow.start_run(experiment_id=0)\n    mlflow.log_param('seed', cfg.seed)\n    print(f'Configured with seed {cfg.seed}!')\n</code></pre></p> <p>Note: Dagshub's server speed is quite limited, having multiple parallel  experiments might result in time our errors. To solve these: <pre><code>os.environ[\"_MLFLOW_HTTP_REQUEST_MAX_RETRIES_LIMIT\"] = \"1000\"\nos.environ[\"_MLFLOW_HTTP_REQUEST_MAX_RETRIES\"] = \"999\"\n</code></pre> Though, this should introduce quite an overhead.</p>"}]}